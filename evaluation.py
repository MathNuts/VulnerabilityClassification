import os
import pickle
import numpy as np
import sklearn.metrics
import matplotlib.pyplot as plt
from sklearn.metrics import PrecisionRecallDisplay
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import roc_curve
from sklearn.metrics import RocCurveDisplay
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

def eval_print(path, *value):
    print(*value)
    f = open("eval/" + path + "/eval.txt", "a")
    f.write(' '.join(str(i) for i in value) + '\n')
    f.close()

def float_print(name, num, path):
    eval_print(path, name + ": %.3f" % num)

# Evaluates for multi-label and a binary label
# Uses val set for thresholding, test for final evaluation
def evaluate(probs_val, true_val, probs_test, true_test, y_columns, path, prauc_path='prauc.png', rocauc_path='rocauc.png', cm_path="cm.png"):
    # Establish if multilabel or binary
    labels = len(probs_val[0])
    # Create folder to hold eval results
    if not os.path.exists("eval/" + path):
        os.makedirs("eval/" + path)
    f = open("eval/" + path + "/eval.txt", "w")
    f.write("")
    f.close()
    
    # Calculate thresholds using validation first
    thresholds = [None] * labels
    for i in range(labels):
        prec, recl, thr = precision_recall_curve(true_val[:,i], probs_val[:,i])
        f1_scores = 2*recl*prec/(recl+prec+1e-7)
        thresholds[i] = thr[np.argmax(f1_scores)]
    eval_print(path, "Thresholds:", thresholds)
    
    
    # Needs looping
    mcc = 0
    f1 = 0
    acc = 0
    pr_displays = [None] * labels
    roc_displays = [None] * labels
    cm_displays = [None] * labels
    
    # Can be calculated automatically
    roc_auc = -1
    roc_auc_w = -1
    pr_auc = -1
    pr_auc_w = -1
    
    # If multilabel
    if labels > 1:
        roc_auc = sklearn.metrics.roc_auc_score(y_true=true_test, y_score=probs_test, average='macro')
        roc_auc_w = sklearn.metrics.roc_auc_score(y_true=true_test, y_score=probs_test, average='weighted')
        pr_auc = sklearn.metrics.average_precision_score(y_true=true_test, y_score=probs_test, average='macro')
        pr_auc_w = sklearn.metrics.average_precision_score(y_true=true_test, y_score=probs_test, average='weighted')


        for i in range(len(probs_test[0])):
            true_class = true_test[:,i]
            probs_class = probs_test[:,i]
            preds_class = probs_class > thresholds[i]
            
            confusion = sklearn.metrics.confusion_matrix(y_true=true_class, y_pred=preds_class)

            f1_class = sklearn.metrics.f1_score(y_true=true_class, y_pred=preds_class)
            f1 += f1_class
            mcc_class = sklearn.metrics.matthews_corrcoef(y_true=true_class, y_pred=preds_class)
            mcc += mcc_class
            acc_class = 100 * (confusion[0][0]+confusion[1][1]) / len(true_class)
            acc += acc_class
            
            roc_auc_class = sklearn.metrics.roc_auc_score(y_true=true_class, y_score=probs_class)
            pr_auc_class = sklearn.metrics.average_precision_score(y_true=true_class, y_score=probs_class)

            eval_print(path, '\n', y_columns[i])
            eval_print(path, confusion)
            float_print('ACC', acc_class, path)
            float_print('AUC ROC', roc_auc_class, path)
            float_print('AUC PR', pr_auc_class, path)
            float_print('MCC', mcc_class, path)
            float_print('F1', f1_class, path)

            prec, recall, _ = precision_recall_curve(true_class, probs_class)
            pr_displays[i] = PrecisionRecallDisplay(precision=prec, recall=recall)

            fpr, tpr, _ = roc_curve(true_class, probs_class)
            roc_displays[i] = RocCurveDisplay(fpr=fpr, tpr=tpr)

            cm = confusion_matrix(true_class, preds_class)
            cm_displays[i] = ConfusionMatrixDisplay(cm)
    else: # If binary case
        probs = probs_test[:,0]
        preds = probs > thresholds[0]
        true_label = true_test[:,0]
        
        print(probs.shape, preds.shape)
        
        confusion = sklearn.metrics.confusion_matrix(y_true=true_label, y_pred=preds)
        acc = 100 * (confusion[0][0]+confusion[1][1]) / len(true_label)
        roc_auc = sklearn.metrics.roc_auc_score(y_true=true_label, y_score=probs)
        pr_auc = sklearn.metrics.average_precision_score(y_true=true_label, y_score=probs)
        f1 = sklearn.metrics.f1_score(y_true=true_label, y_pred=preds)
        mcc = sklearn.metrics.matthews_corrcoef(y_true=true_label, y_pred=preds)
        
        prec, recall, _ = precision_recall_curve(true_label, probs)
        pr_displays[0] = PrecisionRecallDisplay(precision=prec, recall=recall)

        fpr, tpr, _ = roc_curve(true_label, probs)
        roc_displays[0] = RocCurveDisplay(fpr=fpr, tpr=tpr)

        cm = confusion_matrix(true_label, preds)
        cm_displays[0] = ConfusionMatrixDisplay(cm)
        
        
    for i, d in enumerate(pr_displays):
        d.plot(ax=plt.gca(), name=y_columns[i] if labels > 1 else path, linestyle='dashed', linewidth=2, color=['tab:green', 'tab:blue', 'tab:purple', 'tab:red', 'tab:olive'][i])

    plt.legend(loc='upper right', shadow=True)
    plt.grid(True)
    plt.title('Precision-Recall')
    plt.savefig("eval/" + path + '/' + prauc_path, transparent=True, dpi=100)
    plt.clf()
    
    for i, d in enumerate(roc_displays):
        d.plot(ax=plt.gca(), name=y_columns[i] if labels > 1 else path, linestyle='solid', linewidth=2, color=['tab:green', 'tab:blue', 'tab:purple', 'tab:red', 'tab:olive'][i])

    plt.legend(loc='lower right', shadow=True)
    plt.grid(True)
    plt.title('ROC curves')
    plt.savefig("eval/" + path + '/' + rocauc_path, transparent=True, dpi=100)
    plt.clf()
    
    for i, d in enumerate(cm_displays):
        d.plot()
        plt.title('Confusion matrix')
        plt.savefig("eval/" + path + '/' + str(i) + '_' + cm_path, transparent=True, dpi=100)
        plt.clf()
        
    with open("eval/" + path + '/' + 'displays.pkl', 'wb') as file:
        pickle.dump([pr_displays, roc_displays, cm_displays], file)
    
    # Average looped values
    f1 = f1 / labels
    mcc = mcc / labels
    acc = acc / labels
    
    # Stuff last
    eval_print(path, '\n', 'Overall')
    float_print('AUC total', roc_auc, path)
    float_print('AUC weighted', roc_auc_w, path)
    float_print('AUC PR', pr_auc, path)
    float_print('AUC PR weighted', pr_auc_w, path)
    float_print('MCC', mcc, path)
    float_print('F1', f1, path)
    float_print('ACC', acc, path)