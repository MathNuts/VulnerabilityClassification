import h5py
import pandas as pd
import numpy as np
import subprocess
import locale
from subprocess import PIPE
from torchnlp.encoders.text import StaticTokenizerEncoder
import concurrent.futures
import time
import pickle
import re

columns = ['functionSource', 'CWE-119', 'CWE-120', 'CWE-469', 'CWE-476', 'CWE-other']


# ---------------------------------------- Dataset loader


def load_dfs(data_files):
    data = [h5py.File(file, 'r') for file in data_files]
    dfs = [pd.DataFrame() for file in data_files]

    for i in range(len(data_files)):
        for column in columns:
            if column == 'functionSource':
                dfs[i][column] = np.array(data[i][column])
            else:
                dfs[i][column] = np.array(data[i][column]).astype(int)

    return dfs

# ---------------------------------------- Preprocessor 1

def load_variables(var_file):
    identifiers = []
    clang_ids = []
    maps = []

    file = open(var_file, "r")
    clang = False
    for x in file.read().splitlines():
        if not x.strip() or x[0] == "#":
            if "ClangIDs" in x:
                clang = True
        else:
            if clang:
                clang_ids.append(x)
            else:
                splits = x.split()
                if len(splits) > 1:
                    maps.append(splits)
                else:
                    identifiers.append(x)

    return identifiers, maps, clang_ids

def clang_dump_code(code):
    arr = subprocess.run(['clang/bin/clang -Xclang -dump-tokens -E -xc++ -'],
        shell=True,
        input=code,
        encoding=locale.getpreferredencoding(),
        stderr=PIPE
        ).stderr

    return arr

def lexer(dump, variables, log=False):
    identifiers, maps, clang_ids = variables

    lexed = []

    for line in dump.splitlines():
        splits = line.split(" ")
        token = splits[0]
        if token in clang_ids:
            if token == "identifier":
                value = splits[1].split("'")[1]
                if value in identifiers:
                    if log:
                        print("\x1b[32m(CHANGED)\x1b[0m", value)
                    lexed.append(value)
                else:
                    matched = False
                    for m in maps:
                        if value in m[1:]:
                            matched = True
                            lexed.append(m[0])
                            break
                      
                    if not matched:
                        lexed.append(token)
            elif token == "numeric_constant":
                number = splits[1].split("'")[1]
                if number.isdigit():
                    for digit in number:
                        lexed.append(digit)
                else:
                    lexed.append(token)
            else:
                lexed.append(token)
        else:
            if log:
                print("\x1b[31m(REMOVED)\x1b[0m", token)

    return lexed

def augment_code(code):
    return re.sub(r'^(\s)*#', 'hash_', code, flags=re.MULTILINE) # ^#.*\n?

def clang_dump_dfs(dfs):
    clang_dumps = [[] for x in dfs]

    for i, df in enumerate(dfs):        
        functionSources = df['functionSource'].apply(lambda x: augment_code(x.decode('UTF-8')))

        start = time.time()
        with concurrent.futures.ProcessPoolExecutor(max_workers=None) as executor:
            for j in range(len(functionSources)):
                clang_dumps[i].append(executor.submit(clang_dump_code, functionSources[j]))

        print("[Dataset " + str(i) + "] Processed in", time.time() - start, "s")

        clang_dumps[i] = [x.result() for x in clang_dumps[i]]
        
    return clang_dumps

def lex_clang_dumps(clang_dumps, variables):
    lexed = [[] for x in clang_dumps]
    for i, clang_dump in enumerate(clang_dumps):
        lexed[i] = [lexer(x, variables) for x in clang_dump]
    return lexed

def generate_tokenizer(lexed_train, min_occurrences=0):
    encoder = StaticTokenizerEncoder(lexed_train, tokenize=lambda s: s, min_occurrences=min_occurrences)
    return encoder


# ---------------------------------------- Preprocessor 2

def number_splitter(text):
    return re.sub(r"\b(\d+)\b", lambda match : " " + " ".join(list(match.group())) + " ", text)

def space_adder(text):
    return re.sub(r"([()\[\]{}+\-<>=%*])", " \\1 ", text)

def comment_remover(text):
    def replacer(match):
        s = match.group(0)
        if s.startswith('/'):
            return " "
        else:
            return s
    pattern = re.compile(
        r'//.*?$|/\*.*?\*/|\'(?:\\.|[^\\\'])*\'|"(?:\\.|[^\\"])*"',
        re.DOTALL | re.MULTILINE
    )
    return re.sub(pattern, replacer, text)

def code_transform(code):
    return space_adder(number_splitter(comment_remover(code)))


def transform_dfs(dfs, train_df_idx=0):
    dfs_processed = [df.copy() for df in dfs]
    
    for i, df in enumerate(dfs):
        dfs_processed[i].iloc[:,0] = df.iloc[:,0].map(lambda x: code_transform(x.decode('UTF-8')))
    
    return [df['functionSource'] for df in dfs_processed]

def custom_tokenizer(string):
    string = re.sub(r"[\t\n\r]", " ", string)
    string = re.sub(r"\s+", " ", string)
    l = string.split(" ")
    return l

def lex_dfs(dfs, train_df_idx=0):
    function_sets = transform_dfs(dfs, train_df_idx=0)
    lexed_sets = [[], [], []]
    
    for i, functions in enumerate(function_sets):
        for function in functions:
            lexed_sets[i].append(custom_tokenizer(function))
    
    return lexed_sets